{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech=open(r'modibihar.txt','r+').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "# Tokenizing without punctation - 'r\\w+'\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+|.')\n",
    "\n",
    "# tokenize text\n",
    "tokens = tokenizer.tokenize(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of peoples and places (proper nouns) mentioned in the speech\n",
    "persons_things=[]\n",
    "\n",
    "for t in tokens:\n",
    "    if (tokens[(tokens.index(t)-1)]!='.') & ((t.capitalize()==t) | (t.isupper())):\n",
    "        persons_things.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing names of peoples and places from the text\n",
    "tok=[tk for tk in tokens if tk not in persons_things]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making all words lowercase\n",
    "toklow=[x.lower() for x in tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting list of english stopwords,that is,words like am,you etc.\n",
    "stopwords=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "toklow_ns=[to for to in toklow if to not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove dots \n",
    "toklow_ns_nd=[tn for tn in toklow_ns if tn!='.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To lemmatize correctly, we need to find out which words are noun, which verbs etc.\n",
    "# Clasifying words and saving each word and its respective\n",
    "# part of speach into words_and_its_part_of_speech\n",
    "words_and_its_part_of_speech=nltk.pos_tag(toklow_ns_nd)\n",
    "\n",
    "# saving words together with their location\n",
    "words_loc=[(word_loc,length) for word_loc,length in zip(\n",
    "    words_and_its_part_of_speech,list(range(len(words_and_its_part_of_speech)))\n",
    ")]\n",
    "\n",
    "# lemmatizing nouns,verbs and adjectives and storing them to\n",
    "# separate lists, together with their locations\n",
    "\n",
    "words_loc_noun=[]\n",
    "words_loc_verb=[]\n",
    "words_loc_adjective=[]\n",
    "\n",
    "\n",
    "\n",
    "for pair_of_pairs,length in zip(words_loc,list(range(len(toklow)))):\n",
    "    if ((pair_of_pairs[0][1]=='NN') | (pair_of_pairs[0][1]=='NNS')\\\n",
    "        | (pair_of_pairs[0][1]=='NNP') |(pair_of_pairs[0][1]=='NNPS')):\n",
    "        \n",
    "        words_loc_noun.append((nltk.stem.WordNetLemmatizer()\\\n",
    "                            .lemmatize(pair_of_pairs[0][0],pos='n'),length))\n",
    "    \n",
    "    elif ((pair_of_pairs[0][1]=='VB') | (pair_of_pairs[0][1]=='VBN')\\\n",
    "          | (pair_of_pairs[0][1]=='VBG')|(pair_of_pairs[0][1]=='VBD')| \\\n",
    "          (pair_of_pairs[0][1]=='VBP') | (pair_of_pairs[0][1]=='VBZ')):\n",
    "        \n",
    "        words_loc_verb.append((nltk.stem.WordNetLemmatizer()\\\n",
    "                            .lemmatize(pair_of_pairs[0][0],pos='v'),length))\n",
    "        \n",
    "    elif ((pair_of_pairs[0][1]=='JJ') | (pair_of_pairs[0][1]=='JJR')\\\n",
    "          | (pair_of_pairs[0][1]=='JJS')):\n",
    "        words_loc_adjective.append((nltk.stem.WordNetLemmatizer()\\\n",
    "                                 .lemmatize(pair_of_pairs[0][0],pos='a'),length))\n",
    "        \n",
    "\n",
    "# putting all lemmatized words together\n",
    "lemmatized=words_loc_noun+words_loc_verb+words_loc_adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting lemmatized words by their position in the text\n",
    "lemmatized_sorted=sorted(lemmatized,key=lambda tup:tup[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting words from tuples of words and locations\n",
    "lemmatized_words=[ls[0] for ls in lemmatized_sorted]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
